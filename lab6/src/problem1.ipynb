{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 1. In this problem we will use synthetic data sets to explore the bias-variance tradeoff\n",
    "incurred by using regularization.\n",
    "<ul>\n",
    "<li> Generate data of the form:\n",
    "$y = X\\beta+ \\epsilon$\n",
    "where X is an n x p matrix where n = 51, p = 50, and each $X_{ij}  N(0; 1)$ Also, generate\n",
    "the noise according to \u000fi \u0018 N(0; 1=4). Let \f",
    " be the all ones vector (for simplicity).\n",
    "By repeatedly doing this experiment and generating fresh data (fresh X, and y, and hence \u000f {\n",
    "but make that you're not reseting your random seed!) but keeping \f",
    " \f",
    "xed, you will estimate\n",
    "many different solutions, ^ \f",
    ". Estimate the mean and variance of ^ \f",
    ". Note that ^ \f",
    " is a vector,\n",
    "so for this exercise simply estimate the variance of a single component.</li>\n",
    "    <li>Use ridge regression, i.e., `2 regularization. Vary the regularization coe\u000ecient \u0015 = 0:01; 0:1; 1; 10; 100 and repeat the above experiment. What do you observe? As you increase \u0015 is the model becoming more simple or more complex? As you increase \u0015 is performance becoming better or worse? Also compute LOOCV for each \u0015. How does the value of LOOCV, and in particular how it changes as \u0015 varies, compare with what you observe for the explicitly computed\n",
    "        variance?</li>\n",
    "    <li>Read about the Bootstrap, and try to use it to compute the variance (as above), but with a\n",
    "    single copy of the data, rather than with many fresh copies of the data.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our data\n",
    "X = np.random.randn(51,50)\n",
    "beta = np.ones((50,1))\n",
    "epsilon = np.random.normal(loc=0, scale=0.25, size=(51,1))\n",
    "y = np.matmul(X, beta) + epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51, 1)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
