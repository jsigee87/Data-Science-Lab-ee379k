{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "## Who this is written for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2\n",
    "## Finite Dimensional Vector Spaces\n",
    "### Span, Linear Independence, Bases, Dimension\n",
    "\n",
    "#### Linear Combination\n",
    "<body>\n",
    "   <p>\n",
    "     Any vector $x$ s.t. $x$ can be written as:\n",
    "   </p>\n",
    "   <p>\n",
    "     $$a_1v_1 + ... + a_nv_n$$ \n",
    "     $$where \\quad v_1, ... ,v_n \\quad are \\, vectors, \\, and \\quad a_1, ... ,a_n \\in \\pmb{F}$$\n",
    "   </p>\n",
    "   <p>\n",
    "     We think of a linear combination as being a vector itself. This vector is the result of a list of vectors, where each vector in the list is multiplied by a scalar, and they are all added together. Note that formally, we do not require multiplication by scalars, $\\pmb{F}$ could be complex numbers, or some other field.\n",
    "   </p>\n",
    "</body>\n",
    "\n",
    "#### Span\n",
    "\n",
    "<body>\n",
    "   <p>\n",
    "    We can allow multiple linear combinations by taking any scalars for a specific list of vectors. The set of all possible linear combinations for a list of vectors is called the $span$ of that list. Formally, we denote span:\n",
    "   </p> \n",
    "    $$span(v_1,...,v_n)=\\{a_1v_1 + ... + a_nv_n \\,|\\, a_1,...,a_n \\in \\pmb{F} \\}$$\n",
    "   <p>\n",
    "    It turns out that this set of vectors (span) is actually a subspace. It is worth noting that that subspace is the smallest possible subspace that contains all those vectors (there are no \"extra\" vectors). If the span of some vectors <i>equals</i> some vector space, then we can say that list of vectors <b>spans</b> that vector space. Additionally, if we say that a vector space is <i>finite dimensional</i>, that means that by definition, some list of vectors span it.\n",
    "   </p>\n",
    "</body>\n",
    "\n",
    "#### Linear Dependence\n",
    "\n",
    "<body>\n",
    "   <p>\n",
    "     We consider again a linear combination- a list of vectors, each multiplied by some element in our field, and added together. If the only way that we can force this linear combination to equal zero is to set all $a_1,...a_n$ equal to zero, then we can declare the list linearly independent. This is worth thinking about for a second. Another way to say it is, a list of vectors is linearly independent if no vectors in the list are multiples of another (then forcing their combination equal to zero would be trivial by setting all $a_i$ to zero except for the two that are multiples, etc.) Another way to think of it is if a list is linearly independent, then every vector in the span of that list can be uniquely represented by a choice of $a_1,...,a_n$. Therefore, we could think of a specific set of $a_1,...,a_n$ as a vector $\\pmb{a}$, and say that vector gets mapped to a specific vector $\\pmb{x}$ in the span of our list when our list is independent. Notice also that the zero vector then gets mapped to the origin, whereas a dependent list could map some $\\pmb{a}$ with non-zero entries into the origin.\n",
    "   </p>\n",
    "</body>\n",
    "\n",
    "#### Basis\n",
    "\n",
    "<body>\n",
    "   <p>\n",
    "     Suppose we have some list of vectors $v_1,...,v_n$ that is linearly independent, and also spans some vector space <i>V</i>. We can give this list a special name, a <i>basis</i>. We notice that this means we can <i>uniquely</i> write every vector in V as a linear combination of the basis. The main difference between a linearly independent list and a basis is that the basis constrains the length of the list. If we have a basis, we can say that the length of that basis determines the dimension of the vector space that it spans. \n",
    "   </p>\n",
    "   <p>\n",
    "     $$If\\,\\,(v_1,...,v_n)\\,\\,is\\,\\,a\\,\\,basis\\,\\,for\\,\\,V,\\,\\,then\\,\\,dim(V) = n$$\n",
    "   </p>\n",
    "   <p>\n",
    "     With our definition of dimension, we can say that every linearly independent list in a vector space V that has length equal to $dim(V)$ is a basis for V. A last thought is that the basis is the fewest number of linearly independent vectors that can span the space.\n",
    "   </p>\n",
    "</body> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3\n",
    "### Linear Maps\n",
    "<p>\n",
    "   A linear map from some space $V$ to another space $W$ is a function $T:V \\rightarrow W$ . Because we say the map is <i>linear</i>, we require that:\n",
    "</p>\n",
    "<p>\n",
    "    $$T(u + v) = T(u) + T(v) \\quad and \\quad T(\\lambda v) = \\lamba T(v)$$\n",
    "</p>\n",
    "<p>\n",
    "    Given two spaces $V$ and $W$ we can take all the possible maps from $V$ to $W$ and group them together in a set. We call this set of maps $\\mathcal{L}(V,W)$ . Although we are defining maps abstractly, it may be helpful to think of how we normally take these maps to be matrices that move points from one space to different points in another space. \n",
    "</p>\n",
    "<p>\n",
    "    <i> Remark</i>:\n",
    "    Given a basis for $V$ ($v_b$) and a basis for $W$, ($v_w$), the map that takes $v_b$ to $w_b$ is unique! Put another way, for: $$ \\pmb{T}v_b=w_b$$ it turns out that T is unique for each choice of basis $v_b$ and $w_b$.\n",
    "</p>\n",
    "<p>\n",
    "    Since technically a map is just a matrix, it makes sense that we can multiply them together. When multiplying matrices together, it can be thought of as the composition of functions.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5\n",
    "### Diagonalization and Similarity Transforms\n",
    "#### Similarity\n",
    "<p>\n",
    "    We may be interested in two matrices that have the same eigenvalues, and there exists an invertible mapping between them. We call such matrices \"similar\". Notice that two matrices may share eigenvalues, but not be similar, and that similar matrices must be square.\n",
    "</p>\n",
    "<p>\n",
    "    <b>Definition</b>: Similar&#10;\n",
    "</p>\n",
    "<p>\n",
    "    $\\quad$Given $A$ and $B$, when there exists an invertible mapping $P$ s.t. $A\\,=\\,PBP^{-1}$, we say $A$ and $B$ are <i>similar</i>\n",
    "</p>\n",
    "<p>\n",
    "    Notice that:$$A\\,=\\,PBP^{-1}\\quad\\quad and \\,\\,that \\quad\\quad\\,B\\,=\\,P^{-1}AP$$ So we can say that we change $A$ into $P^{-1}AP$. Something interesting is happening here though. Consider that $A \\in \\mathcal{L}(V,W)$. Then what can we say about the domains of our mappings $P$, $B$, and $P^{-1}$?\n",
    "</p>\n",
    "<p>\n",
    "    Well, first we assume that $A$ goes from $V$ to $W$. Looking at $P^{-1}AP$, we must conclude that it also is a map from $V$ to $W$. However, the only way this is possible is if $P$ has   $range\\,=\\,V$ , due to the composition of   $A\\,\\circ\\,P$. Then we conclude that $P^{-1}$ has   $domain\\,=\\,V$ . However,   $A\\,\\circ\\,P$   has   $range\\,=\\,W$,   and so   $P^{-1}\\circ(AP)$     only makes sense when $P^{-1}$ has   $domain\\,=\\,V$. Therefore, our discussion only makes sense when   $V\\,=\\,W$ . And so it must be the case that   $A \\in \\mathcal{L}(V,V)$. (Do not be ashamed to draw this out with blobs of shapes representing V and W in the same way we learn functions in basic set theory).\n",
    "</p>\n",
    "<p>\n",
    "    This is all a long winded way to say that the existence of a similarity transform between two mappings implies that those mappings are <i>linear operators</i>. In fact, since similar matrices are linear operators, it turns out that they actually represent <i>the same transformation with respect to a different basis</i>. Therefore these operators are mapping objects within a vector space to different objects in the same vector space, AND their similar operators do the same thing, but from a different point of view. Another interesting note is that there exists an <i>entire family</i> of matrices similar to $A$.\n",
    "</p>\n",
    "\n",
    "#### Diagonalization\n",
    "\n",
    "<p>\n",
    "    There is a lot more to say about this in the context of eigenvalues and eigenvectors, and so our goal is to come up with useful factorizations of a transformation, i.e. generate \"good\" $P$ and $B$ matrices. One immediately useful $B$ matrix would be an upper triangular form of $A$ that displayed its eigenvalues. In fact, we can construct a diagonal matrix that displays only the eigenvalues of $A$. This process is called <b>diagonalization</b> and is usually denoted by $A\\,=\\,P^{-1}DP$, with $D$ being the diagonal matrix of eigenvalues.\n",
    "</p>\n",
    "<p>\n",
    "    <b>Definition</b>: Diagonalizable&#10;\n",
    "</p>\n",
    "<p>\n",
    "    $\\quad$$A$ is diagonalizable iff   $A$   has   $n$   linearly independent eigenvectors, and   $A$   is $n$ x $n$\n",
    "</p>\n",
    "<p>\n",
    "    \n",
    "</p>\n",
    "<p>\n",
    "    \n",
    "</p>\n",
    "<p>\n",
    "    \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
